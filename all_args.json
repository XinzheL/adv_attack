{
// added vias `options.get_training_parser(default_task='translation')` 
"no_progress_bar": false, 
 // for log
 "log_interval": 1000, "log_format": null, 

 "tensorboard_logdir": "", "seed": 1, "cpu": false, 
 
 // fp16
 "fp16": false, 
 "memory_efficient_fp16": false, 
 "fp16_init_scale": 128, 
 // scale
 "fp16_scale_window": null, 
 "fp16_scale_tolerance": 0.0, 
 "min_loss_scale": 0.0001, 
 "threshold_loss_scale": null, 
 
 "user_dir": null, "empty_cache_freq": 0, 
 
 "criterion": "cross_entropy", 

 // tokenizer & BPE
 "tokenizer": null, "bpe": "subword_nmt", 

 "optimizer": "nag", 
 "lr_scheduler": "fixed", 
 "task": "translation", 
 "num_workers": 1, 
 "skip_invalid_size_inputs_valid_test": false, 
 "max_tokens": null, 
 "max_sentences": null, 
 "required_batch_size_multiple": 8, 

 // dataset  
 "dataset_impl": null, "train_subset": "train", "valid_subset": "valid", 
 
 // validate setting
 "validate_interval": 1, "fixed_validation_seed": null, "disable_validation": false, "max_tokens_valid": null, "max_sentences_valid": 1, 
 
 // multi-gpu parallel training 
 "curriculum": 0, "distributed_world_size": 1, "distributed_rank": 0, 
 "distributed_backend": "nccl", "distributed_init_method": null, 
 "distributed_port": -1, "device_id": 0, "distributed_no_spawn": false, 
 "ddp_backend": "c10d", "bucket_cap_mb": 25, "fix_batches_to_gpus": false, 
 "find_unused_parameters": false, "fast_stat_sync": false, 
 
 // #48
 "arch": "transformer_vaswani_wmt_en_de_big", 
 "max_epoch": 0, "max_update": 0, "clip_norm": 25, "sentence_avg": false, 
 "update_freq": [1], "lr": [0.25], "min_lr": -1, "use_bmuf": false, 
 "save_dir": "checkpoints",

 "restore_file": "wmt16.en-de.joined-dict.transformer/model.pt", 
 "reset_dataloader": true, 
 "reset_lr_scheduler": true, 
 "reset_meters": true, 
 "reset_optimizer": true, 
 "optimizer_overrides": "{}", 
 
 "save_interval": 1, "save_interval_updates": 0,
 "keep_interval_updates": -1, "keep_last_epochs": -1, 
 
 "no_save": false, 
 "no_epoch_checkpoints": false, 
 "no_last_checkpoints": false, 
 "no_save_optimizer_state": false, 
 
 "best_checkpoint_metric": "loss", 
 "maximize_best_checkpoint_metric": false, 

// manually added in `all_attack_utils.py`
 "interactive_attacks": true, 

// model-specific argumetns from transformer.py
 "no_token_positional_embeddings": false, 
 "no_cross_attention": false, 
 "cross_self_attention": false, 
 "layer_wise_attention": false, 
 "encoder_layerdrop": 0, 
 
 "decoder_layerdrop": 0, 
 "encoder_layers_to_keep": null, 
 "decoder_layers_to_keep": null, 

// criterion-specific arguments 

// tokenizer-specific arguments

//  bpe arguments from `fairseq.data.encoders.subword_nmt_bpe.Subword_nmt`
 "bpe_codes": "wmt16.en-de.joined-dict.transformer/bpecodes", 
 "bpe_separator": "@@", 
 
// optimizer-specific arguments from `fairseq.optim.nag.FairseqNAG`
 "momentum": 0.99, "weight_decay": 0.0, 
 
//  lr_scheduler-specific arguments from `fairseq.optim.lr_scheduler.fixed_schedule.FixedSchedule`
 "force_anneal": null, "lr_shrink": 0.1, 
 "warmup_updates": 0, 
 
//  task-specific arguments from fairseq.tasks.translation.TranslationTask
// data arg used for setting up dictionary in task.setup_task() and load data split in task.load_dataset()
 "data": "wmt16.en-de.joined-dict.newstest2014/", 
 "source_lang": "en", "target_lang": "de", 
 "lazy_load": false, "raw_text": false, "load_alignments": false, 
 "left_pad_source": true, "left_pad_target": false, 
 "max_source_positions": 1024, "max_target_positions": 1024, 
 "upsample_primary": 1, 

// apecific transformer arch
 "encoder_embed_dim": 1024, 
 "encoder_ffn_embed_dim": 4096, 
 "encoder_attention_heads": 16, 
 "encoder_normalize_before": false, 
 "decoder_embed_dim": 1024, 
 "decoder_ffn_embed_dim": 4096, 
 "decoder_attention_heads": 16, 
 "dropout": 0.3, 

//  transformer.base_architecture()
 "encoder_embed_path": null, 
 "encoder_layers": 6, 
 "encoder_learned_pos": false, 
 "decoder_embed_path": null, 
 "decoder_layers": 6,
 "decoder_normalize_before": false, 
 "decoder_learned_pos": false,
 
 "attention_dropout": 0.0, 
 "activation_dropout": 0.0, 
 "activation_fn": "relu", 
 "adaptive_softmax_cutoff": null, 
 "adaptive_softmax_dropout": 0, 
 "share_decoder_input_output_embed": false, 
 "share_all_embeddings": false, 
 "adaptive_input": false, 
 "decoder_output_dim": 1024, 
 "decoder_input_dim": 1024, 
 "no_scale_embedding": false, 
 "layernorm_embedding": false, 

 
 "path": "wmt16.en-de.joined-dict.transformer/model.pt", 
 "beam": 1}